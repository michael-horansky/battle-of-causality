\documentclass[12pt]{article}
\usepackage[a4paper, total={6.5in, 8in}]{geometry}
\usepackage{xargs}
\usepackage{amsmath,amssymb}
\usepackage{physics}


\begin{document}

	\title{Elo-like rating system for asymmetric games}
	\author{Michal Horanský}
	\maketitle
	
	\section{Introduction}	
	
	Suppose we are designing a zero-sum game which allows players to play competitive one-on-one matches on custom boards (by board we here refer to a specific starting configuration of the game). We wish to implement a rating system to serve a purpose akin to that of Elo rating for chess players. This rating system has to satisfy the same principal property of Elo rating, which is as follows: \textit{the rating difference has to reflect the expected outcome of the game.} That is to say, we assume that each player has their "true strength", and the difference of true strengths of two players facing each other governs the expected outcome of the match. We then adjust player ratings based on their games so that \textit{if a player would always perform according to his true strength, his rating would converge to a constant value regardless on the true strength of his opponents.}
	
	The Elo rating in chess achieves this by identifiyng the difference between the measured outcome of a match and the expected outcome as the measured fault of the player's rating, and the post-match rating adjustment is technically a gradient descent towards the equilibrium. If player $A$ scores $S_A$ points in a match against player $B$, his new rating $R_A'$ will be calculated as such:
	\begin{equation} \label{rating_adjustment}		
	R_A' = R_A + K\cdot \left(S_A - P_A(R_A - R_B)\right)
	\end{equation}
where $P_A(R_A - R_B)$ is the probability player $A$ wins the game\footnote{Equivalently: the expected value of player $A$'s score.} given the rating difference, and $K$ is the speed of the descent towards equilibrium (high $K$ means volatile system, low $K$ means rigid system). We see that \textit{any} choice of rating system--i.e. the specific function $P_A$, which links true strength and rating--satisfies the principal condition if Eq. (\ref{rating_adjustment}) is used. An important consequence is that Eq. (\ref{rating_adjustment}) automatically conserves the total number of Elo points in circulation, since $S_B = 1 - S_A$ (as we assume a zero-sum game) and $P_A(R_A - R_B) = 1 - P_B(R_B - R_A)$, since $P$ as a probability value must sum up to $1$ over all participants.

	\section{Asymmetric boards and handicap}

	In our case, the problem is complicated by the asymmetry of the board. Suppose we parametrise this asymmetry with a real parameter $h$, which quantifies the advantage player $A$ has over player $B$. Then, we need to devise a rating system such that Player $A$'s win probability in a match is given by the function $P^{(a)}_A(R_A - R_B, h)$ (the superscript serves to distinguish the asymmetric probability from the standard Elo-like win probability). We have a freedom of choice, but let us take the most straightforward way and interpret $h$ as a handicap applied to the rating difference like so:
	\begin{equation}\label{asymmetric_expectancy}
	P^{(a)}_A(R_A - R_B, h) = P_A(R_A - R_B + h)
	\end{equation}
	In other words, the "true" rating difference is context-dependent, and is calculated as the sum of the difference of player ratings plus the handicap.
	
	\subsection{Inferring the handicap from prior data}
	
	The problem is that we do not know the value of $h$ for a given board. Suppose, however, that we keep track of all games ever played for every board in the following format:
	\begin{table}[h]
	\begin{center} 
	\begin{tabular}{ c | c }
	\multicolumn{2}{c}{board $i$} \\
	\hline
 	$\Delta R^i_1$ & $S^i_1$ \\ 
 	$\Delta R^i_2$ & $S^i_2$ \\ 
 	\multicolumn{2}{c}{$\vdots$} \\ 
 	$\Delta R^i_j$ & $S^i_j$    \\
 	\multicolumn{2}{c}{$\vdots$}
	\end{tabular}
	\end{center}
	\caption{Data about games played on a specific board.}\label{game_table}
	\end{table}
	
	where $\Delta R^i_j$ is the rating difference $R_A - R_B$ between the players who played the $j$-th game on board $i$, and $S^i_j$ is player $A$'s achieved score in that game (standardly $1$ for win, $0$ for loss, $1/2$ for draw).
	
	What we now want to find out when players $A$ and $B$ sit down to play a game on board $i$ whose historical games have been recorded in \ref{game_table} is the probability of player $A$ winning given the rating difference \textit{and} the previous games. In other words, we are looking for the conditional probability
	$$P^{(a)}_A(A\text{ wins } | \text{ data})$$
	We start by considering every possible value of $h$ and identifying its contribution to the conditional probability as the product of the probability given the value of $h$ multiplied by the probability of that value being correct given the data:
	\begin{equation} \label{prob_convolution}
	P^{(a)}_A(A\text{ wins } | \text{ data}) = \int P_A(R_A - R_B + h) P(h\mid\text{data}) \text{d}h
	\end{equation}
	where we used Eq. (\ref{asymmetric_expectancy}) to express the probability conditioned by a specific handicap value. As for $P(h\mid\text{data})$, we can now turn to Bayes' theorem:
	\begin{equation} \label{bayes}
	P(h\mid\text{data}) = \frac{P(\text{data}\mid h)P(h)}{P(D)}
	\end{equation}
	Here, $P(\text{data}\mid h)$ is the likelihood function which gives us a probability of measuring the dataset in Tab. \ref{game_table} given a specific value of $h$. As such, it is a function of $h$ and can be evaluated easily if we assume that every game played is independent of every other game:
	\begin{eqnarray} \label{handicap_likelihood}
	P(\text{data}\mid h) = \prod_j^{\text{not draws}} P(S^i_j\mid\Delta R^i_j, h)\\
	P(S^i_j\mid\Delta R^i_j, h)=\begin{cases} 
      P_A(\Delta R^i_j + h) & S^i_j = 1\text{ (Player A wins)} \\
      1 - P_A(\Delta R^i_j + h) & S^i_j = 0\text{ (Player B wins)}
   \end{cases}
	\end{eqnarray}
	Notice how we ignored datapoints which ended in a draw. This is because in our underlying symmetric model, we have no way of assigning probability to the draw output. This requires a new free parameter, as per Elo-Davidson rating systems (see further sections). Here we just assume that drawn games have nothing to say about the handicap value.
	
	\subsection{Handicap prior}
	
	Coming back to Eq. (\ref{bayes}), we see two more unexplained terms. $P(D)$ is simply a normalization factor for the likelihood function (which, as you can see, is not normalized with respect to $h$), and as such is simply equal to
	\begin{equation}\label{likelihood_normalization}
	P(D) = \int P(\text{data}\mid h)P(h) \text{d}h
	\end{equation}
	so that $P(h\mid\text{data})$ has integral norm $1$ w.r.t. $h$. Finally $P(h)$ is the prior probability distribution of the handicap--in other words, the probability that a random board has handicap $h$. We could use a non-informative prior and rely on a large dataset to accurately retrieve the posterior handicap distribution, but there is a better way. Since we are already storing information about all games across \textit{all boards}, we can simply inspect all the other boards to estimate a prior standard deviation on $h$ and assume e.g. a normal distribution around zero\footnote{For the first few boards, we will indeed need to use a non-informative prior, e.g. an initial guess on the standard deviation $\sigma_h$, which then iteratively gets better as we update this value with more and more boards and more games on each board, and converges to the true, equilibirum value.}. We do not actually need to re-calculate the prior standard deviation every time from all of the games for all the boards; we can just keep a list of mean values of $h$ measured for existing boards and estimate $\sigma_h$ from this aggregate dataset.
	
	\subsection{Expected score}
	
	We can now write down player $A$'s expected score as a function of the rating difference and the data for this board and handicaps for previous boards:
	\begin{align} \label{expectancy_based_on_data}
	P^{(a)}_A(A\text{ wins}\mid\text{data}) &= \int P_A(R_A - R_B + h) \frac{P(\text{data}\mid h)P(h)}{\int P(\text{data}\mid h')P(h') \text{d}h'} \text{d}h \\
	P(\text{data}\mid h) &= \prod_j^{\text{not draws}} P(S^i_j\mid\Delta R^i_j, h)\nonumber \\
	P(S^i_j\mid\Delta R^i_j, h)&=\begin{cases} 
      P_A(\Delta R^i_j + h) & S^i_j = 1\text{ (Player A wins)} \\
      1 - P_A(\Delta R^i_j + h) & S^i_j = 0\text{ (Player B wins)}
   \end{cases} \nonumber\\
   P(h) &= \frac{1}{\sigma_h\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{h}{\sigma_h}\right)^2} \nonumber
	\end{align}
	
	The new expected value of $h$ to be added to the list which is used to calculate $\sigma_h$ is simply
	\begin{equation}
	E[h] = E[P(h\text{ } | \text{ data})] = \frac{\int hP(\text{data } | \text{ }h)P(h) \text{d}h}{\int P(\text{data } | \text{ }h)P(h) \text{d}h}
	\end{equation}
	and the update to the rating based on the match outcome is obtainable by substituing Eq. (\ref{expectancy_based_on_data}) into Eq. (\ref{rating_adjustment}).
	
	\subsection{Dynamical step-size and retroactive rating adjustment}
	
	Note that the history of games played on a specific board retroactively adjusts the rating gain from each of those games, as the likelihood function becomes more and more accurate in its description of the true handicap value. Consequently, the rating of a player gets retroactively adjusted after each game played \textit{after} his match on the same board. This sounds like a bug, but it is actually a feature, since early players should \textit{not} suffer from lack of information about the board's fairness. Following this philosophy, there is one more improvement ready to be made to incentivize players to use boards with small datasets: retrospectively adjusting the $K$ parameter with regard to our certainty about the board's fairness. To keep things simple, this could be estimated purely by the size of our dataset about the given board, i.e. $N^i=$ number of non-drawn games recorded on board $i$. Then, we would choose a function $K(N^i)$ such that as $N^i\to 0$, $K\to 0$, and as $N^i\to\infty$, $K\to K_\text{max}$. I propose a simple hyperbolic curve:
	\begin{equation}
	K(N^i)=32\frac{N^i}{10+N^i}
	\end{equation}
	This function is designed so that $K$ converges to $32$, a value used for low-rated players in chess, when played on boards about whose unfairness we are certain about, and it reaches half of that value after $10$ games played on the board. These parameters can, of course, be changed to reflect your game's needs.
	
	It is customary to note the specific function for expected score which is used for Elo rating in chess:
	$$P_A(R_A-R_B)=\frac{1}{1+10^{(R_B-R_A)/s}}$$
	where $s$ is typically chosen to be $400$, so that a difference in rating of $400$ points means player $A$ is ten times as likely to win as player $B$.
	
	\section{Including drawn games}
	In the previous section, we have sketched the idea for inferring information about the handicap inherent to an asymmetric board using data about the games played previously (on that and all other boards). However, we have deliberately omitted drawn games when estimating the likelihood of a given handicap value, as we had no way of stating the probability that a game between two players with a specific handicap will be drawn.
	
	\subsection{Parametrising the probability of a draw}
	
	There is a way to expand our model to include the probability of drawing and then go through an analogous process to determine the expected score for player $A$. To do this, we begin with the Elo-Davidson model, which introduces a free parameter $\kappa$, which is related to the probability a game between two players will be drawn. The probabilities of the three outcomes in the Elo-Davidson model are as follows:
	\begin{eqnarray} \label{elo_davidson}
	P(A\text{ wins}\mid \Delta R,\kappa) &=& \sigma(\Delta R,\kappa)\\
	P(B\text{ wins}\mid \Delta R,\kappa) &=& \sigma(-\Delta R,\kappa)\\
	P(\text{draw}\mid \Delta R,\kappa) &=& \kappa\sqrt{\sigma(\Delta R,\kappa)\sigma(-\Delta R,\kappa)}\\
	\text{where}\quad \sigma(\Delta R, \kappa)&=&\frac{10^{\Delta R/s}}{10^{-\Delta R/s}+\kappa+10^{\Delta R/s}}
	\end{eqnarray}
	Now, suppose we once again have a dataset about all the previous games in the form $(\Delta R^i_j,\text{ outcome})$ for the $j$-th game played on the $i$-th board. We can first write two equations analogous to Eq. (\ref{prob_convolution}):
	\begin{eqnarray} \label{prob_convolution_2}
	P(A\text{ wins}\mid\text{data}) &=& \iint P(A\text{ wins}\mid \Delta R + h, \kappa) P(h,\kappa\mid\text{data}) \text{d}h\text{d}\kappa\\
	P(\text{draw}\mid\text{data}) &=& \iint P(\text{draw}\mid \Delta R + h, \kappa) P(h,\kappa\mid\text{data}) \text{d}h\text{d}\kappa
	\end{eqnarray}
	where, naturally,
	$$P(B\text{ wins}\mid\text{data})=1-P(A\text{ wins}\mid\text{ data})-P(\text{draw}\mid\text{ data})$$
	and thus we do not need to calculate the probability of player $B$ winning.
	
	As for the posterior probability distribution for $h,\kappa$, we once again turn to Bayes' theorem:
	\begin{equation}
	P(h, \kappa\mid\text{data}) = \frac{P(\text{data}\mid h, \kappa)P(h,\kappa)}{\iint P(\text{data}\mid h, \kappa)P(h,\kappa)\text{d}h\text{d}\kappa}
	\end{equation}
	The likelihood function is now straightforward to write down and uses all datapoints, including drawn games:
	\begin{align}
	P(\text{data}\mid h, \kappa) &= \prod_j^{\text{board }i} P(\Delta R^i_j,\text{ outcome}\mid h,\kappa)\\
	P(\Delta R^i_j,\text{ outcome}\mid h,\kappa) &= \begin{cases}
		\sigma(\Delta R + h, \kappa) & \text{if player }A\text{ won}\\
		\sigma(-\Delta R - h, \kappa) & \text{if player }B\text{ won}\\
		\kappa\sqrt{\sigma(\Delta R + h, \kappa)\sigma(-\Delta R - h, \kappa)} & \text{if draw}
	\end{cases} \nonumber
	\end{align}
	\subsection{Prior for $\kappa$} \label{kappa_prior}
	\subsubsection{Assumptions for separability and lack of correlations}
	As for the prior probability distribution for $h,\kappa$, a seasoned statistician with a large dataset could go to town. We here propose the simplest solution, once again aggregating data about all the other boards. Let us assume that the probability that a random board has a specific handicap and that it has a specific probability of a draw occuring are uncorrelated. Then, we can separate the prior like so:
	\begin{equation}
	P(h,\kappa) = P(h)P(\kappa)
	\end{equation}
	As for the prior $P(h)$, we use just the same approach as in the previous sections. For $P(\kappa)$, we can use a similar approach, given one more assumption. For the sake of numerical simplicity, let us assume that the average game played on a random board has a zero rating difference. Then, the probability of a draw is simply
	\begin{equation} \label{zero_diff_kappa}
	P(\text{draw}\mid\kappa)=\frac{\kappa}{2+\kappa}
	\end{equation}
	For the $i$-th board, we can estimate its probability of a draw as the ratio of the number of draws and the number of all games played on this board:
	$$P^i(\text{draw})=\frac{N^i(\text{draws})}{N^i(\text{all games})}$$
	and then
	$$\kappa^i=2\frac{N^i(\text{draws})/N^i(\text{all games})}{1-N^i(\text{draws})/N^i(\text{all games})}$$
	This is purely a simple estimate for the value of $\kappa$ for a given board, and it could be improved by including the rating differences in Eq. (\ref{zero_diff_kappa}) and then performing either a numerical fit, or another Bayesian iteration. However, for the sake of numerical simplicity, we shall make the zero-difference assumption in this calculation, which is justified by the fact the matchmaking algorithm tries to minimize rating difference, and so for most matches the probability of a draw will be roughly constant. Naturally, this means we are slightly underestimating the value of $\kappa$, but as this estimate is only used for a prior, it matters little.
	
	After collecting the values of $\kappa^i$ for all the other boards, we can quickly calculate their mean value $\langle\kappa\rangle=N(\text{draws})/N(\text{all games})$. As will be demonstrated below, the mean value is all we need to find the prior $P(\kappa)$ in this simplified model.
	\subsubsection{Probability distribution of $p$ and $\kappa$}
	As mentioned above, \textit{in the prior} we assume that the probability of a game being drawn is correlated to neither its handicap nor its rating difference. Since all the games are played independently of each other, if the true probability of a game being drawn is $p$, then the probability of $N_d$ games out of $N$ being drawn follows the binomial distribution. As the number of $N$ goes to infinity, the probability distribution of the ratio $\hat{p}=N_d/N$ approaches a continuous distribution, which can be identified as the Beta distribution:
	\begin{equation} \label{beta_dist}
	P(\hat{p},p)=\frac{\Gamma(N+2)}{\Gamma(N\hat{p}+1)\Gamma(N(1-\hat{p})+1)}p^{N\hat{p}}(1-p)^{N(1-\hat{p})}
	\end{equation}
	We have measured the value of $\hat{p}$ and are interested in the likelihood of a given value of the true probability $p$. Since Eq. (\ref{beta_dist}) is already normalized with respect to $p$ for a fixed $\hat{p}$ (as $N\to\infty$), we can interpret it directly as the probability density function of $p$. 
	
	We are interested in the probability distribution $P_\kappa$ of the $\kappa=\frac{2\hat{p}}{1-\hat{p}}$ parameter. For this, we employ the rule for transforming probability distributions:
	\begin{equation}\label{lotus}
	\text{If }Y=G(X)\text{ and }X\text{ follows }f_X(x)\text{, then }Y\text{ follows }f_Y(y)=f_X(G^{-1}(y))\left|\frac{\text{d}}{\text{d}y}G^{-1}(y)\right|
	\end{equation}
	Substituing Eq. (\ref{beta_dist}) into Eq. (\ref{lotus}) yields the probability density function of $\kappa$ given a measured mean probability of drawing a game $\langle P(\text{draw})\rangle$:
	\begin{equation} \label{draw_prob}
	P_\kappa(\hat{p},\kappa)=\frac{\Gamma(N+2)}{\Gamma(N\hat{p}+1)\Gamma(N(1-\hat{p})+1)}\frac{2^{N(1-\hat{p})+1}\kappa^{N\hat{p}}}{(2+\kappa)^{N+2}}
	\end{equation}
	This probability density function has all we need: it is undefined for $\kappa<0$ and normalized on the non-negative reals, and its expected value is $\frac{2\hat{p}}{1-\hat{p}}$, agreeing with the initial measurement of drawing probability. As such, it is a useful prior for the $\kappa$ parameter.
	
	The total prior then becomes
	\begin{equation}\label{prior_2}
	P(h,\kappa) = \frac{1}{\sigma_h\sqrt{2\pi}}\exp\left[-\frac{1}{2}\left(\frac{h}{\sigma_h}\right)^2\right]\frac{\Gamma(N+2)}{\Gamma(N\hat{p}+1)\Gamma(N(1-\hat{p})+1)}\frac{2^{N(1-\hat{p})+1}\kappa^{N\hat{p}}}{(2+\kappa)^{N+2}}
	\end{equation}
	\subsubsection{Approximate version of the $\kappa$ prior}	
	For very large $N$, Eq. (\ref{beta_dist}) is subject to the Central Limit Theorem, since it describes the distribution of the sum of a large number of independent measurements. Therefore
	\begin{equation}
	\lim_{N\to\infty} P(\hat{p},p) = \mathcal{N}\left(\hat{p},\frac{\hat{p}(1-\hat{p})}{N}\right)
	\end{equation}
	We observe the mean $\langle p\rangle=\hat{p}$ and standard deviation $\sigma_p=\sqrt{\frac{\hat{p}(1-\hat{p})}{N}}$ agree with the binomial distribution for $N_d$.
	
	Applying Eq. (\ref{lotus}) once again, we see that $\kappa$ does not follow a normal distribution even in the large $N$ limit:
	\begin{equation}
	\lim_{N\to\infty} P(\hat{p},\kappa)=\frac{2}{(2+\kappa)^2\sigma_p\sqrt{2\pi}}\exp\left[-\frac{1}{2}\left(\frac{\kappa-\frac{2\langle p\rangle}{1-\langle p\rangle}}{\frac{2+\kappa}{1-\langle p\rangle}\sigma_p}\right)^2\right]
	\end{equation}
	However, if we assume $\kappa\to 0$, which is reasonable if and only if our game has a low proabbility of a draw, this approximates a normal distribution with the properties
	\begin{eqnarray}
	\langle\kappa\rangle &=& \frac{2\langle p\rangle}{1-\langle p\rangle}\\
	\sigma_\kappa &=& \frac{2}{1-\langle p\rangle}\sigma_p
	\end{eqnarray}
	This final approximation allows us to write the total prior $P(h,\kappa)$ as a two-dimensional normal distribution.
	
	\subsection{Score calculation from win and draw probability}
	
	As for the final step, we evaluate player $A$'s expected score
	\begin{equation}
	E_A^{(A)}(\Delta R\mid\text{data}) = P(A\text{ wins}\mid\text{data}) + \frac{1}{2}P(\text{draw}\mid\text{data})
	\end{equation}
	Using this expected score value and the dynamic value of $K(N^i(\text{games}))$, we obtain the\\ Elo\nobreakdash-Davidson\nobreakdash-Horanský rating system, characterised by the rating adjustment to player $A$ after the game:
	\begin{equation}
	R_A' = R_A + K(N^i)(S_A - E_A^{(A)}(\Delta R\mid\text{data}))
	\end{equation}
	\section{Algorithm summary for the Elo-Davidson-Horanský rating system}
	The following algorithm can be readily applied when building any asymmetric one-vs-one zero-sum game which permits wins, losses, and draws:
	\begin{enumerate}
	\item We first choose the value of $s$, which simply sets the scale of the rating. If every new player starts with a rating of $1000$, the chess value of $s=400$ is reasonable. This value's interpretation is as follows: for every $s$ points of rating difference, the ratio of the two players' win probabilities multiplies tenfold.
	\item Before any boards are created, we need to estimate a starting prior for the handicaps and draw probabilities. This requires some knowledge of the specific game we are designing, but the following values should be good benchmarks for any application:
	\begin{itemize}
	\item For the handicap, we estimate that a carefully-made board will rarely make one player more than twice as likely to win as their equally-rated opponent. Hence a good standard deviation estimate for $h$ will be $\sigma_h\approx s\log_{10}(2)\approx 120$.
	\item For the draw parameter, first estimate the probability of a draw on a symmetric board for two equally-rated players. If, for example, this estimate if $1/10$, the corresponding expected value of $\langle\kappa\rangle$ is $2\cdot\frac{1/10}{1-1/10}\approx 0.2$. As per Sec. \ref{kappa_prior}, the mean fully specifies the prior distribution from Eq. (\ref{prior_2}). Since we have no data, we can set $N=1$ in the prior for the first calculation, maximizing the uncertainty.
	\end{itemize}
	We will be using these low-information priors until our dataset grows large enough (let's say $N(\text{all games}) > 30$ for $P(\kappa)$ and $N(\text{boards with at least }5\text{ games played}) > 5$ for $P(h)$)
	\item If two players rated $R_A$ and $R_B$, respectively, play a game on board $m$, we follow this procedure:
	\begin{enumerate}
	\item If this was the first game played on this board, the step-size $K$ is zero, and the ratings do not change. Record the game and exit algorithm.
	\item Otherwise, we calculate the likelihood function $P(\text{data}\mid h,\kappa)$ from all previous games played on this board, \textit{including the one that was just played}. The reason for that is we will be retrospectively adjusting the rating changes from all of the games played on this board, and thus they should all use the same statistic. The uniformness of this outweights the trouble of one badly-behaved datapoint for each game to be re-rated.
	\item We choose the priors for $P(h)$ and $P(\kappa)$, either based on the data already collected, or, if the datasets are too small, just opting in for the initial approximations prepared in earlier steps, and combine them to form the two-dimensional prior $P(h,\kappa)$. For this, we use the aggregate data for all the boards \textit{excluding the one that was just played on.} The reason for this is that the priors are meant to estimate the probability this board has a specific handicap and drawing probability, and thus using its own data would be ill-formed.
	\item We calculate the normalization factor $\iint P(h,\kappa)P(\text{data}\mid h,\kappa)\text{d}h\text{d}\kappa$, and by dividing the integrand by the integral we obtain the posterior $P(h,\kappa\mid \text{data})$.
	\item We calculate the new step-size for the board, $K$.
	\item Now, we will readjust the rating for every game played on this board. For every game, we calculate the probability of player $A$ winning and the game being drawn, respectively, from the posterior and the rating difference. From this, we obtain player $A$'s expected score. The new, updated value of the rating adjustment for player $A$ will be the new step-size multiplied by the difference between player $A$'s obtained score and his expected score. Player $B$'s updated rating adjustment will be the of equal value and opposite sign. We retroactively update each player's rating as the sum of the initial value of $1000$ and all the adjustments across all the games they played (but this can be simplified as only the rating adjustments from games played on this specific board have changed).
	\end{enumerate}
	\item We update the expected handicap value for the board in the aggregate dataset using the new posterior: $\langle h\rangle=\iint hP(h,\kappa\mid\text{data})\text{d}h\text{d}\kappa$. This value will be used to calculate the priors for games played only on other boards, not this one.
	\end{enumerate}
	
	\section{Implementing retroactive rating on a large-scale dataset}
	
	The aforementioned algorithm produces a "correct" statistic in the sense that its repeated application converges to the true values of player rating and board handicap. However, it is massively inefficient on a large scale (in terms of player/board/game count), because it is applied retroactively to all the games played on the affected board and, implicitly, to all the other games played at a later date than any of the recalculated games, since retroactively changing the rating of a game affects the ratings going into all subsequent games for the affected players. Hence, in this short section, we propose a "good-enough" algorithm which is designed with these features in mind as the top priorities:
	\begin{itemize}
	\item \textbf{Correctness}: Applying this algorithm will lead to the same results as applying the full-scale recalculation with the correct statistic.
	\item \textbf{Efficiency}: This algorithm is largely inexpensive, and only requires one expensive calculation per a fixed time interval.
	\item \textbf{Responsiveness}: Using this algorithm means that, after submitting a new concluded game, the rating of the engaged players and the handicap of the used board are updated immediately.
	\item \textbf{Symmetry}: When recalculating player rating adjustments for games on a single board, they all take the same step size and values of $h,\kappa$, which means their time-ordering does not matter in the approximating limit of small step size, where the rating adjustments can be taken to commute.
	\end{itemize}
	The algorithm is sectioned into two parellel processes:
	\begin{enumerate}
	\item \textbf{Immediate updates}: On submitting a new concluded game, the rating and handicap update is calculated according to the full statistic and applied to the "hot" values of these parameters for the players and the board. However, no retrospective recalculation is applied across the dataset. The date of the earliest game played on the affected board is stored; next time this happens, only the earlier of the two dates is stored.
	\item \textbf{Housekeeping}: At regular intervals (e.g. nightly), a recalculation for all games which have occured later than the date stored is performed, and the "hot" values of ratings and handicaps are adjusted to match the fully correct statistic.
	\end{enumerate}
	The downside of this algorithm is that the "hot" values only approximate the correct statistic; however, they are constantly adjusted by the housekeeping process, and so they remain largely correct. The upside is that large-scale recalculation, which has to happen at least sometimes if we require our model to be correct with regard to the statistic, does not occur based on user input, and thus its overall demand on computing power does not scale with the user base.
	
	Another method to limit the time complexity of the housekeeping process, which in theory makes it independent on the number of boards, is to stop updating the handicap value on any given board once a specific condition is satisfied (accurate: if the handicap has not changed by a significant amount for the last $N$ new datapoints; approximate: once the number of datapoints exceeds $N$, with $N$ being arbitrary for both approaches). This means that, in theory, as new boards are added, old boards either "solidify" and no longer warrant retroactive recalculation (so that playing on such a board simply updates the rating but does not store its earliest game date), or stop being used for games, and thus no longer enlarge the scale of housekeeping recalculation. This second approximation is useful to be implemented on large-scale userbases, but before doing that, ensure you have enough data to estimate reasonable values of the parameter $N$ for the handicap-calculation-freezing condition.
	
	\subsection{Implementation}
	The following is a mockup on how to implement the Elo-Davidson-Horanský rating system in your game, assuming your database has relational tables USER, BOARD, and GAME.
	\subsubsection{Approaches to forming priors}
	If we formed the prior on $\kappa$ based on all the games played, we would need to recalculate every single game ever played for every housekeeping procedure. It would be better and more accurate to only form the prior from the data for the specific board, as different boards will have different values of $kappa$. For small dataset values, the prior should be informed mostly by the initial estimate $\kappa_0$ (and the corresponding initial estimate of the drawing probability $\hat{p}_0$), and as the number of games played on the board ($N$) grows, the prior will be more and more informed by the measured value of $\hat{p}=N_d/N$. This can be achieved e.g. with the following formula:
	\begin{equation}
	\hat{p}^\text{prior}=\frac{\hat{p}_0+\frac{N}{\nu_p}\hat{p}}{1+\frac{N}{\nu_p}}=\frac{\nu_p\hat{p}_0+N_d}{\nu_p+N}
	\end{equation}
	where $\nu_p$ is an arbitrary parameter which determines how quickly the measured value of $\hat{p}$ overtakes the initial estimate as the dataset on the board grows. Specifically, for $N=\nu_p$ games measured on the board, the value $\hat{p}^\text{prior}$ is exactly the arithmetic average of the initial and the measured value.
	
	The same issue holds for estimating the prior on $h$, which is determined by its expected value $\langle h\rangle$ and its standard deviation $\sigma_h$, as it is modelled as a normal distribution. We do not want to assume that even the prior for the handicap depends on the handicaps of other boards (which would also make the model less numerically stable and more complex), so we use similar techniques for estimating these parameters for the prior depending only on the initial estimates and the hot values:
	\begin{eqnarray}
	\langle h\rangle^\text{prior} &=& \frac{\frac{N}{\nu_h}\langle h\rangle^\text{hot}}{1+\frac{N}{\nu_h}}=\frac{N\langle h\rangle^\text{hot}}{\nu_h+N}\\
	\sigma_h^\text{prior}&=&\frac{\sigma_{h,0}+\frac{N}{\nu_h}\sigma_h^\text{hot}}{1+\frac{N}{\nu_h}}
	\end{eqnarray}
	with $\nu_h$ having an analogous meaning to $\nu_p$.
	
	There is no reason why we should two different scales for the priors on $h$ and $p$, so we can take $\nu_p=\nu_h=\nu$, where $\nu$ is the rigidity of the model.
	
	\subsubsection{Database requirements}
	Table USER has a column RATING\_HOT, which is the hot value of the user's rating. Similarly, table BOARD has columns HANDICAP\_HOT, STD\_HANDICAP\_HOT, which are the hot values of the board's handicap and its standard deviation, column KAPPA\_HOT, which is the hot value of the draw parameter $\kappa$, and column STEP\_SIZE, which is the hot value of the step size $K$. Finally, table GAME has column R\_A\_ADJUSTMENT, which is the adjustment to player $A$'s rating due to that game.
	
	As the model has a few arbitrary parameters, it is useful to have a table PARAMETERS: KEY, VALUE, in which we store the following values:
	\begin{itemize}
	\item Initial rating of a new player, $R_0$ (in my application $1000$)
	\item The rating difference scale, $s$ (in my application $400$)
	\item Initial estimate of $\hat{p}_0$ (in my application $1/10$)
	\item Initial estimate of $\sigma_{h,0}$ (in my application $120$)
	\item Rating adjustment step scale $K_0$ (in my application $32$)
	\item Game count scale for estimating $\hat{p}_\text{prior}, h_\text{prior}, \sigma_{h,\text{prior}}$, i.e. the model rigidity $\nu$ (in my application $10$)
	\item Condition on dataset size for a board to stop updating its handicap, $N_\text{freeze}$. (Can be set to NULL to prevent handicap freezing.)
	\item The volatile value of the earliest datapoint scheduled for recalculation. By default, this is NULL (i.e. unknown).
	\end{itemize}
	
	To oversee the housekeeping process, it is also useful to have a table HOUSEKEEPING\_LOGS: D\_INITIATED, TIME\_TAKEN, GAMES\_AFFECTED, USERS\_AFFECTED, BOARDS\_AFFECTED, GAMES\_TOTAL, USERS\_TOTAL, BOARDS\_TOTAL, which records the state of the total dataset and the dataset's fraction subject to recalculation, and allows the administrator to monitor trends in demand on the server's computation time.
	
	\subsubsection{Realisation of efficient algorithm}
	The algorithm constitutes two processes: immediate updates and housekeeping. Immediate updates are triggered on every submission of a new concluded game, where a particular game is characterised by the two engaged players, the outcome (winning player or draw), and the board utilised. Housekeeping is triggered at specific intervals, or manually by administrator, and does \textit{not} adjust handicap and draw-parameter values, only player ratings.
	
	\begin{itemize}
	\item \textbf{Immediate updates}. On new submission of a concluded game:
		\begin{enumerate}
		\item Consider the hot handicap for the board, $h^\text{hot}$, its hot standard deviation, $\sigma_h^\text{hot}$, the hot draw factor for the board, $\kappa^\text{hot}$, and the hot ratings for each of the two players, $R_{A/B}^\text{hot}$, as well as the total number of games played on this board $N$ and total number of drawn games on this board $N_d$ (both excluding the newly submitted datapoint).
		\item Find the prior estimate for the probability of drawing a game on this board, $\hat{p}^\text{prior}$, and the prior estimate for the handicap and its standard deviation for this board, $\sigma_h^\text{prior}$, using the model parameters and considered values like so:
		\begin{eqnarray}
		\hat{p}^\text{prior}&=&\frac{\hat{p}_0+\frac{N}{\nu}\hat{p}^\text{hot}}{1+\frac{N}{\nu}}\qq{where}\hat{p}^\text{hot}=\frac{\kappa^\text{hot}}{\kappa^\text{hot}+2}\\
		\langle h\rangle^\text{prior} &=&\frac{N\langle h\rangle^\text{hot}}{\nu_h+N}\\
	\sigma_h^\text{prior}&=&\frac{\sigma_{h,0}+\frac{N}{\nu_h}\sigma_h^\text{hot}}{1+\frac{N}{\nu_h}}
		\end{eqnarray}
		\item Then, the prior $P(h, \kappa)$ is like so:
		\begin{align*}
		P(h,\kappa) =& \frac{1}{\sigma_h^\text{prior}\sqrt{2\pi}}\exp\left[-\frac{1}{2}\left(\frac{h-h^\text{prior}}{\sigma_h^\text{prior}}\right)^2\right]\\
		&\cdot\frac{\Gamma(N+2)}{\Gamma(N\hat{p}^\text{prior}+1)\Gamma(N(1-\hat{p}^\text{prior})+1)}\frac{2^{N(1-\hat{p}^\text{prior})+1}\kappa^{N\hat{p}^\text{prior}}}{(2+\kappa)^{N+2}}
		\end{align*}
		We can omit the normalisation factors, as the posterior will be renormalised explicitly, and thus the effective log prior is
		\begin{align*}
		\log{P(h,\kappa)} = \text{cons.}-\frac{1}{2}\left(\frac{h-h^\text{prior}}{\sigma_h^\text{prior}}\right)^2+N\hat{p}^\text{prior}\log{\kappa}-(N+2)\log(\kappa+2)
		\end{align*}
		\item The log-likelihood function for the board (evaluated on all games including the one just submitted) is like so:
		\begin{equation}
		\log{L(\text{data}\mid h,\kappa)}=\sum_{i\text{th game}}\log{P_G(\Delta R_i, \text{ outcome} \mid h, \kappa)}
		\end{equation}
		where $\Delta R_i=R_{A,i}-R_{B,i}$ and the single game log-probability function is
		\begin{equation}
		\log{P_G(\Delta R_i,\text{ outcome}\mid h,\kappa)} = \begin{cases}
			\log{\sigma(\Delta R + h, \kappa)} & \text{if player }A\text{ won}\\
			\log{\sigma(-\Delta R - h, \kappa)} & \text{if player }B\text{ won}\\
			\begin{aligned}
   				\log \kappa + \tfrac{1}{2}\bigl(
        			\log \sigma(\Delta R + h, \kappa) \\
        			+ \log \sigma(-\Delta R - h, \kappa)
     			\bigr)
  			\end{aligned}
  				& \text{if draw}
		\end{cases}
		\end{equation}
		which can be explicitly expressed as
		\begin{equation}
		P_G(...)= -\log(10^{\frac{\Delta R+h}{s}}+10^{-\frac{\Delta R+h}{s}}+\kappa)+\begin{cases}
			\frac{\Delta R+h}{s}\log{10} & \text{if player }A\text{ won}\\
			-\frac{\Delta R+h}{s}\log{10} & \text{if player }B\text{ won}\\
			+0 & \text{if draw}
		\end{cases}
		\end{equation}
		\item Then the unnormalised posterior becomes
		\begin{equation}
		P(h,\kappa\mid\text{data})\propto\exp(\log{P(h,\kappa)}+\log{L(\text{data}\mid h,\kappa)})
		\end{equation}
		Evaluate this explicitly without cancelling the logarithms, because numerical evaluation of $\log{P(h,\kappa)},\log{L(\text{data}\mid h,\kappa)}$ does not suffer from the possibility of numerical underflow.
		\item Normalise the posterior with $\frac{1}{Z}$ by numerically integrating
		\begin{equation}
		Z = \int_{h=-\infty}^{\infty}\int_{\kappa=0}^{\infty} \exp(\log{P(h,\kappa)}+\log{L(\text{data}\mid h,\kappa)}) \dd h\dd\kappa
		\end{equation}
		\item Calculate the step-size for ratings, $K=K_0\frac{N}{\nu+N}$.
		\item Calculate the probability of player $A$ winning and probability of a draw:
		\begin{eqnarray*}
		P(A\text{ wins}) &=& \frac{1}{Z}\int_{h=-\infty}^{\infty}\int_{\kappa=0}^{\infty} \sigma(\Delta R+h,\kappa)\exp(\log{P(h,\kappa)}+\log{L(\text{data}\mid h,\kappa)}) \dd h\dd\kappa\\
		P(\text{draw}) &=& \iint \kappa\sqrt{\sigma(\Delta R+h,\kappa)\sigma(-\Delta R-h,\kappa)}P(h,\kappa\mid\text{data}) \dd h\dd\kappa
		\end{eqnarray*}
		Then, calculate player $A$'s expected score:
		\begin{equation*}
		\langle S_A \rangle = P(A\text{ wins}) + \frac{1}{2}P(\text{draw})
		\end{equation*}
		Then, calculate the rating adjustment for both players:
		\begin{eqnarray*}
		\delta R_A &=& K(S_A-\langle S_A \rangle)\qq{where}S_A =
			\begin{cases}
			1 & \text{if player }A\text{ won}\\
			0 & \text{if player }B\text{ won}\\
			\frac{1}{2} & \text{if draw}
			\end{cases}\\
		\delta R_B &=& -\delta R_A
		\end{eqnarray*}
		Update the hot values of player ratings with these values. Also, save the value of $\delta R_A$ to the column R\_A\_ADJUSTMENT in the recorded game's datarow.
		\item Calculate the new expected value of the handicap and its standard deviation:
		\begin{eqnarray*}
		\langle h\rangle &=& \iint h P(h,\kappa\mid\text{data}) \dd h\dd\kappa\\
		\langle h^2\rangle &=& \iint h^2 P(h,\kappa\mid\text{data}) \dd h\dd\kappa\\
		\sigma_h &=& \sqrt{\langle h^2\rangle - \langle h\rangle^2}
		\end{eqnarray*}
		Calculate the new expected value of $\kappa$:
		\begin{equation*}
		\langle \kappa\rangle = \iint \kappa P(h,\kappa\mid\text{data}) \dd h\dd\kappa
		\end{equation*}
		Update the hot values to be equal to these newly calculated values. This includes the new step size $K$.
		\item Unless the freezing condition is satisfied ($N>N_\text{freeze}$), note down the date of the earliest game played on this board, and overwrite the global earliest date scheduled for recalculation.
		\end{enumerate}
	\item \textbf{Housekeeping}. Regularly:
		\begin{enumerate}
		\item Reset the values of RATING\_HOT for every user to the default value $R_0$.
		\item Consider all games recorded \textit{earlier} than the date scheduled for readjustment. By summing up their values of R\_A\_ADJUSTMENT (with signs flipped according to the player's role) for each player, we calculate the hot values of player ratings for each player as they were right before the earliest game scheduled for readjustment.
		\item Then, for each game, ordered by time of submission, we recalculate the rating adjustment for both players while taking the hot handicap, $\kappa$, and step size value as the \textit{true} values, like so:
			\begin{enumerate}
			\item Take the hot handicap for this board, $h^\text{hot}$, and interpret it as the true value $h$. Take the hot draw-parameter for this board, $\kappa^\text{hot}$, and also interpret it as the true value $\kappa$. Take the hot step size of the board, $K^\text{hot}$, and set the step size $K$ equal to it. Consider the hot ratings of the two players $R_{A/B}^\text{hot}$.
			\item Calculate the effective rating difference $r$, which takes into account the handicap value:
			\begin{equation*}
			r=R_A-R_B+h
			\end{equation*}
			\item Calculate the expected score of player $A$ like so:
			\begin{equation*}
			\langle S_A \rangle = P(A\text{ wins})+\frac{1}{2}P(\text{draw})=\frac{10^{r/s}+\kappa/2}{10^{r/s}+10^{-r/s}+\kappa}
			\end{equation*}
			\item Calculate the rating adjustment for the players like so:
			\begin{eqnarray*}
			\delta R_A &=& K(S_A-\langle S_A \rangle)\qq{where}S_A =
				\begin{cases}
				1 & \text{if player }A\text{ won}\\
				0 & \text{if player }B\text{ won}\\
				\frac{1}{2} & \text{if draw}
				\end{cases}\\
			\delta R_B &=& -\delta R_A
			\end{eqnarray*}
			Store the value of $\delta R_A$ into R\_A\_ADJUSTMENT and update the hot ratings of both players for the next game's recalculation.
			\end{enumerate}
		
		\item After recalculating the rating adjustment for every game, we terminate the program without adjusting handicap or draw-parameter values.
		\end{enumerate}
	\end{itemize}
	
	
\end{document}